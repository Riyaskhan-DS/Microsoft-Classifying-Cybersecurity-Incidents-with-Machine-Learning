{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "72f29347-d437-40dd-a2fb-3564ac6aa3bf",
   "metadata": {},
   "source": [
    "# Importing necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2f8afd87-3c93-43f6-89b0-8baa5edf9cf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting lightgbm\n",
      "  Downloading lightgbm-4.5.0-py3-none-win_amd64.whl.metadata (17 kB)\n",
      "Requirement already satisfied: numpy>=1.17.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from lightgbm) (1.26.4)\n",
      "Requirement already satisfied: scipy in c:\\users\\user\\anaconda3\\lib\\site-packages (from lightgbm) (1.13.1)\n",
      "Downloading lightgbm-4.5.0-py3-none-win_amd64.whl (1.4 MB)\n",
      "   ---------------------------------------- 0.0/1.4 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/1.4 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.0/1.4 MB 435.7 kB/s eta 0:00:04\n",
      "   - -------------------------------------- 0.1/1.4 MB 656.4 kB/s eta 0:00:03\n",
      "   --- ------------------------------------ 0.1/1.4 MB 798.9 kB/s eta 0:00:02\n",
      "   ----- ---------------------------------- 0.2/1.4 MB 1.2 MB/s eta 0:00:02\n",
      "   -------- ------------------------------- 0.3/1.4 MB 1.3 MB/s eta 0:00:01\n",
      "   ----------- ---------------------------- 0.4/1.4 MB 1.4 MB/s eta 0:00:01\n",
      "   -------------- ------------------------- 0.5/1.4 MB 1.6 MB/s eta 0:00:01\n",
      "   ----------------- ---------------------- 0.6/1.4 MB 1.6 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 0.7/1.4 MB 1.8 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 0.9/1.4 MB 1.8 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 1.0/1.4 MB 1.9 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 1.1/1.4 MB 2.0 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 1.2/1.4 MB 2.0 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 1.4/1.4 MB 2.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.4/1.4 MB 2.0 MB/s eta 0:00:00\n",
      "Installing collected packages: lightgbm\n",
      "Successfully installed lightgbm-4.5.0\n"
     ]
    }
   ],
   "source": [
    "!pip install lightgbm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c1272fa0-60b0-4e55-9209-1d9a74df5c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV, StratifiedKFold, train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.combine import SMOTEENN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fa2c7cbb-13fd-4062-984d-561193438587",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the encoded train and test data\n",
    "train_data = joblib.load('encoded_train_data.joblib', mmap_mode='r')\n",
    "test_data = joblib.load('encoded_test_data.joblib', mmap_mode='r')\n",
    "\n",
    "# Separating features (X) and target variable (y)\n",
    "X = train_data.drop(columns=['IncidentGrade'])\n",
    "y = train_data['IncidentGrade']\n",
    "\n",
    "# Check if target variable is continuous or categorical\n",
    "if y.dtype.kind in 'iuf':  # Numeric check\n",
    "    # Define bins if you want to classify continuous values\n",
    "    num_classes = 3  # Example: 3 categories, adjust as needed\n",
    "    try:\n",
    "        y_binned = pd.cut(y, bins=num_classes, labels=False)  # Binning for classification\n",
    "        is_classification = True\n",
    "        y = y_binned\n",
    "    except Exception as e:\n",
    "        print(\"Could not bin continuous variable. Treating as regression target.\")\n",
    "        is_classification = False\n",
    "else:\n",
    "    is_classification = True  # Categorical target\n",
    "\n",
    "# Splitting the data (80:20)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y if is_classification else None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c184973b-ce1e-49f3-bb50-b45c398c6a25",
   "metadata": {},
   "source": [
    "# Comparing Machine Learning Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "79248003-9587-478d-8711-efcf8e99401c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: Logistic Regression\n",
      "Accuracy: 0.6460084900128643\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.92      0.73    775107\n",
      "           1       0.70      0.15      0.25    390976\n",
      "           2       0.75      0.62      0.67    628025\n",
      "\n",
      "    accuracy                           0.65   1794108\n",
      "   macro avg       0.68      0.56      0.55   1794108\n",
      "weighted avg       0.67      0.65      0.60   1794108\n",
      "\n",
      "Confusion Matrix:\n",
      "[[713727  14564  46816]\n",
      " [247262  58590  85124]\n",
      " [231163  10170 386692]]\n",
      "--------------------------------------------------\n",
      "Model: Random Forest Classifier\n",
      "Accuracy: 0.7092566333799303\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.85      0.76    775107\n",
      "           1       0.67      0.45      0.54    390976\n",
      "           2       0.77      0.70      0.73    628025\n",
      "\n",
      "    accuracy                           0.71   1794108\n",
      "   macro avg       0.71      0.67      0.68   1794108\n",
      "weighted avg       0.71      0.71      0.70   1794108\n",
      "\n",
      "Confusion Matrix:\n",
      "[[655738  51499  67870]\n",
      " [148646 177107  65223]\n",
      " [151698  36689 439638]]\n",
      "--------------------------------------------------\n",
      "Model: Decision Tree Classifier\n",
      "Accuracy: 0.7082739723584087\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.85      0.76    775107\n",
      "           1       0.67      0.45      0.54    390976\n",
      "           2       0.78      0.69      0.73    628025\n",
      "\n",
      "    accuracy                           0.71   1794108\n",
      "   macro avg       0.71      0.66      0.68   1794108\n",
      "weighted avg       0.71      0.71      0.70   1794108\n",
      "\n",
      "Confusion Matrix:\n",
      "[[661718  50256  63133]\n",
      " [152662 176126  62188]\n",
      " [157167  37982 432876]]\n",
      "--------------------------------------------------\n",
      "Model: Gradient Boosting Classifier\n",
      "Accuracy: 0.6532934472172244\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.95      0.73    775107\n",
      "           1       0.78      0.16      0.27    390976\n",
      "           2       0.78      0.59      0.67    628025\n",
      "\n",
      "    accuracy                           0.65   1794108\n",
      "   macro avg       0.72      0.57      0.56   1794108\n",
      "weighted avg       0.70      0.65      0.61   1794108\n",
      "\n",
      "Confusion Matrix:\n",
      "[[739216   9755  26136]\n",
      " [250229  64059  76688]\n",
      " [250958   8263 368804]]\n",
      "--------------------------------------------------\n",
      "Model: XGBoost Classifier\n",
      "Accuracy: 0.6859966066702785\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.93      0.75    775107\n",
      "           1       0.76      0.28      0.40    390976\n",
      "           2       0.79      0.65      0.71    628025\n",
      "\n",
      "    accuracy                           0.69   1794108\n",
      "   macro avg       0.73      0.62      0.62   1794108\n",
      "weighted avg       0.71      0.69      0.66   1794108\n",
      "\n",
      "Confusion Matrix:\n",
      "[[717202  15614  42291]\n",
      " [216807 107914  66255]\n",
      " [203776  18613 405636]]\n",
      "--------------------------------------------------\n",
      "Model: LightGBM Classifier\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.119111 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 310\n",
      "[LightGBM] [Info] Number of data points in the train set: 717643, number of used features: 155\n",
      "[LightGBM] [Info] Start training from score -0.842540\n",
      "[LightGBM] [Info] Start training from score -1.520670\n",
      "[LightGBM] [Info] Start training from score -1.047484\n",
      "Accuracy: 0.684919191040896\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.94      0.75    775107\n",
      "           1       0.75      0.27      0.40    390976\n",
      "           2       0.81      0.63      0.71    628025\n",
      "\n",
      "    accuracy                           0.68   1794108\n",
      "   macro avg       0.73      0.61      0.62   1794108\n",
      "weighted avg       0.72      0.68      0.66   1794108\n",
      "\n",
      "Confusion Matrix:\n",
      "[[725503  15985  33619]\n",
      " [223386 107289  60301]\n",
      " [212073  19925 396027]]\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Sample for quick training\n",
    "X_train_subsample = X_train.sample(frac=0.1, random_state=42)\n",
    "y_train_subsample = y_train.loc[X_train_subsample.index]\n",
    "\n",
    "# Define models based on classification or regression\n",
    "if is_classification:\n",
    "    models = {\n",
    "        'Logistic Regression': LogisticRegression(max_iter=1000, random_state=42),\n",
    "        'Random Forest Classifier': RandomForestClassifier(n_jobs=-1, random_state=42),\n",
    "        'Decision Tree Classifier': DecisionTreeClassifier(random_state=42),\n",
    "        'Gradient Boosting Classifier': GradientBoostingClassifier(random_state=42),\n",
    "        'XGBoost Classifier': XGBClassifier(n_jobs=-1, random_state=42),\n",
    "        'LightGBM Classifier': LGBMClassifier(n_jobs=-1, random_state=42),\n",
    "    }\n",
    "else:\n",
    "    models = {\n",
    "        'Random Forest Regressor': RandomForestRegressor(n_jobs=-1, random_state=42),\n",
    "        'Gradient Boosting Regressor': GradientBoostingRegressor(random_state=42),\n",
    "        'XGBoost Regressor': XGBRegressor(n_jobs=-1, random_state=42),\n",
    "        'LightGBM Regressor': LGBMRegressor(n_jobs=-1, random_state=42),\n",
    "    }\n",
    "\n",
    "# Train and evaluate models\n",
    "for model_name, model in models.items():\n",
    "    print(f'Model: {model_name}')\n",
    "    \n",
    "    model.fit(X_train_subsample, y_train_subsample)\n",
    "    y_pred = model.predict(X_val)\n",
    "    \n",
    "    if is_classification:\n",
    "        # Classification evaluation\n",
    "        accuracy = accuracy_score(y_val, y_pred)\n",
    "        report = classification_report(y_val, y_pred)\n",
    "        cm = confusion_matrix(y_val, y_pred)\n",
    "        print(f'Accuracy: {accuracy}')\n",
    "        print('Classification Report:')\n",
    "        print(report)\n",
    "        print('Confusion Matrix:')\n",
    "        print(cm)\n",
    "    else:\n",
    "        # Regression evaluation\n",
    "        mse = mean_squared_error(y_val, y_pred)\n",
    "        r2 = r2_score(y_val, y_pred)\n",
    "        print(f'Mean Squared Error: {mse}')\n",
    "        print(f'R^2 Score: {r2}')\n",
    "    \n",
    "    print('-' * 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0b8fe309-3b66-4b34-817c-4b5809eda07b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparison Table:\n",
      "              Model  Accuracy  Macro-F1 Score  Precision  Recall\n",
      "Logistic Regression    0.6460            0.55       0.67    0.65\n",
      "      Random Forest    0.7093            0.68       0.71    0.71\n",
      "      Decision Tree    0.7083            0.68       0.71    0.71\n",
      "  Gradient Boosting    0.6533            0.56       0.70    0.65\n",
      "            XGBoost    0.6860            0.62       0.73    0.69\n",
      "           LightGBM    0.6849            0.62       0.72    0.68\n",
      "\n",
      "Best Model Based on Macro-F1 Score (and Accuracy in case of a tie):\n",
      "Model             Random Forest\n",
      "Accuracy                 0.7093\n",
      "Macro-F1 Score             0.68\n",
      "Precision                  0.71\n",
      "Recall                     0.71\n",
      "Name: 1, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Creating a report data\n",
    "report = {\n",
    "    'Model': ['Logistic Regression', 'Random Forest', 'Decision Tree', 'Gradient Boosting', 'XGBoost', 'LightGBM'],\n",
    "    'Accuracy': [0.6460, 0.7093, 0.7083, 0.6533, 0.6860, 0.6849],\n",
    "    'Macro-F1 Score': [0.55, 0.68, 0.68, 0.56, 0.62, 0.62],\n",
    "    'Precision': [0.67, 0.71, 0.71, 0.70, 0.73, 0.72],\n",
    "    'Recall': [0.65, 0.71, 0.71, 0.65, 0.69, 0.68]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(report)\n",
    "\n",
    "print(\"Comparison Table:\")\n",
    "print(df.to_string(index=False))\n",
    "\n",
    "best_models_with_max_f1 = df[df['Macro-F1 Score'] == df['Macro-F1 Score'].max()]\n",
    "\n",
    "if len(best_models_with_max_f1) > 1:\n",
    "    best_model = best_models_with_max_f1.loc[best_models_with_max_f1['Accuracy'].idxmax()]\n",
    "else:\n",
    "    best_model = df.loc[df['Macro-F1 Score'].idxmax()]\n",
    "\n",
    "print(\"\\nBest Model Based on Macro-F1 Score (and Accuracy in case of a tie):\")\n",
    "print(best_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86adf6d9-4e74-494c-a5e1-ea7529f59652",
   "metadata": {},
   "source": [
    "# Applying SMOTE to the training data for class imbalance and doing hyperparameter tuning for best result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f3f043-94a5-4e64-8d1c-985c99fcf592",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 5 candidates, totalling 15 fits\n"
     ]
    }
   ],
   "source": [
    "# Loading the encoded train data\n",
    "train_data = joblib.load('encoded_train_data.joblib', mmap_mode='r')\n",
    "\n",
    "# Separating the features (X) and target variable (y)\n",
    "X = train_data.drop('IncidentGrade', axis=1)\n",
    "y = train_data['IncidentGrade'].copy()  # Make a copy of y to avoid the writeable issue\n",
    "\n",
    "# If the target variable (y) is continuous, you can bin it into categories\n",
    "# For example, creating 3 bins for a \"low\", \"medium\", \"high\" classification problem\n",
    "y_binned = pd.cut(y, bins=3, labels=[\"low\", \"medium\", \"high\"])\n",
    "\n",
    "# Converting to numeric and handling NaN values\n",
    "X = X.apply(pd.to_numeric, errors='coerce')\n",
    "X = X.dropna(axis=1)\n",
    "\n",
    "# Splitting the data (80:20)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y_binned, test_size=0.2, random_state=42, stratify=y_binned)\n",
    "\n",
    "# Downsampling the training data to 2% for quicker processing\n",
    "X_train_sampled, _, y_train_sampled, _ = train_test_split(X_train, y_train, train_size=0.02, stratify=y_train, random_state=42)\n",
    "\n",
    "# If there are boolean columns, convert them to integers\n",
    "if X_train_sampled.select_dtypes(include=['bool']).shape[1] > 0:\n",
    "    X_train_sampled = X_train_sampled.astype(int)\n",
    "\n",
    "# Applying SMOTE for multi-class classification (default strategy balances all classes equally)\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_resampled, y_train_resampled = smote.fit_resample(X_train_sampled, y_train_sampled)\n",
    "\n",
    "# Hyperparameters for RandomizedSearchCV\n",
    "param_dist = {\n",
    "    'n_estimators': [50, 75],\n",
    "    'max_depth': [10, 20, None],\n",
    "    'min_samples_split': [2, 5],\n",
    "    'min_samples_leaf': [1, 2],\n",
    "    'bootstrap': [True, False]\n",
    "}\n",
    "\n",
    "# Random Forest Classifier\n",
    "rf = RandomForestClassifier(random_state=42, n_jobs=-1)\n",
    "\n",
    "random_search = RandomizedSearchCV(estimator=rf, param_distributions=param_dist, n_iter=5,\n",
    "                                   cv=3, verbose=1, random_state=42, n_jobs=-1)\n",
    "\n",
    "# Fitting the Randomized Search with resampled training data\n",
    "random_search.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "# Best parameters and model\n",
    "best_rf = random_search.best_estimator_\n",
    "\n",
    "# Evaluating on validation data\n",
    "y_pred = best_rf.predict(X_val)\n",
    "\n",
    "# Printing the results\n",
    "print(\"Best Hyperparameters:\", random_search.best_params_)\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_val, y_pred))\n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_val, y_pred))\n",
    "\n",
    "# Saving the tuned model\n",
    "joblib.dump(best_rf, \"rf_smote_tuned_model.joblib\")\n",
    "print(\"Model saved as rf_smote_tuned_model.joblib\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95de82fc-33f7-4e6b-8556-8a921ba8943d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
